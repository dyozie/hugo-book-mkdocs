<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pivotal HDB 2.3.0 Documentation on Pivotal HDB</title>
    <link>https://example.org/</link>
    <description>Recent content in Pivotal HDB 2.3.0 Documentation on Pivotal HDB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://example.org/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About GPORCA</title>
      <link>https://example.org/query/gporca/query-gporca-optimizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-optimizer/</guid>
      <description>In HAWQ, you can use GPORCA or the legacy query optimizer.
Note: To use the GPORCA query optimizer, you must be running a version of HAWQ built with GPORCA, and GPORCA must be enabled in your HAWQ deployment.
These sections describe GPORCA functionality and usage:
 Overview of GPORCA
GPORCA extends the planning and optimization capabilities of the HAWQ legacy optimizer.
 GPORCA Features and Enhancements
GPORCA includes enhancements for specific types of queries and operations:</description>
    </item>
    
    <item>
      <title>About HAWQ Query Processing</title>
      <link>https://example.org/query/hawqqueryprocessing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/hawqqueryprocessing/</guid>
      <description>This topic provides an overview of how HAWQ processes queries. Understanding this process can be useful when writing and tuning queries.
Users issue queries to HAWQ as they would to any database management system. They connect to the database instance on the HAWQ master host using a client application such as psql and submit SQL statements.
Understanding Query Planning and Dispatch After a query is accepted on master, the master parses and analyzes the query.</description>
    </item>
    
    <item>
      <title>Accessing External SQL Databases with JDBC (Beta)</title>
      <link>https://example.org/pxf/jdbcpxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/jdbcpxf/</guid>
      <description>Some of your data may already reside in an external SQL database. The PXF JDBC plug-in reads data stored in SQL databases including MySQL, ORACLE, PostgreSQL, and Hive.
This section describes how to use PXF with JDBC, including an example of creating and querying an external table that accesses data in a MySQL database table.
Prerequisites Before accessing external SQL databases using HAWQ and PXF, ensure that:
 The JDBC plug-in is installed on all cluster nodes.</description>
    </item>
    
    <item>
      <title>Accessing HBase Data</title>
      <link>https://example.org/pxf/hbasepxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/hbasepxf/</guid>
      <description>Prerequisites Before trying to access HBase data with PXF, verify the following:
 The /etc/hbase/conf/hbase-env.sh configuration file must reference the pxf-hbase.jar. For example, /etc/hbase/conf/hbase-env.sh should include the line:
export HBASE_CLASSPATH=${HBASE_CLASSPATH}:/usr/lib/pxf/pxf-hbase.jar  Note: You must restart HBase after making any changes to the HBase configuration.
 PXF HBase plug-in is installed on all cluster nodes.
 HBase and ZooKeeper jars are installed on all cluster nodes.
  Syntax To create an external HBase table, use the following syntax:</description>
    </item>
    
    <item>
      <title>Accessing HDFS File Data</title>
      <link>https://example.org/pxf/hdfsfiledatapxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/hdfsfiledatapxf/</guid>
      <description>HDFS is the primary distributed storage mechanism used by Apache Hadoop applications. The PXF HDFS plug-in reads file data stored in HDFS. The plug-in supports plain delimited and comma-separated-value format text files. The HDFS plug-in also supports the Avro binary format.
This section describes how to use PXF to access HDFS data, including how to create and query an external table from files in the HDFS data store.
Prerequisites Before working with HDFS file data using HAWQ and PXF, ensure that:</description>
    </item>
    
    <item>
      <title>Accessing Hive Data</title>
      <link>https://example.org/pxf/hivepxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/hivepxf/</guid>
      <description>Apache Hive is a distributed data warehousing infrastructure. Hive facilitates managing large data sets supporting multiple data formats, including comma-separated value (.csv), RC, ORC, and parquet. The PXF Hive plug-in reads data stored in Hive, as well as HDFS or HBase.
This section describes how to use PXF to access Hive data. Options for querying data stored in Hive include:
 Querying Hive tables via PXF&amp;rsquo;s integration with HCatalog Creating an external table in PXF and querying that table  Prerequisites Before accessing Hive data with HAWQ and PXF, ensure that:</description>
    </item>
    
    <item>
      <title>Accessing JSON File Data</title>
      <link>https://example.org/pxf/jsonpxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/jsonpxf/</guid>
      <description>The PXF JSON plug-in reads native JSON stored in HDFS. The plug-in supports common data types, as well as basic (N-level) projection and arrays.
To access JSON file data with HAWQ, the data must be stored in HDFS and an external table created from the HDFS data store.
Prerequisites Before working with JSON file data using HAWQ and PXF, ensure that:
 The PXF HDFS plug-in is installed on all cluster nodes.</description>
    </item>
    
    <item>
      <title>Apache HAWQ System Requirements</title>
      <link>https://example.org/requirements/system-requirements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/requirements/system-requirements/</guid>
      <description>Follow these guidelines to configure each host machine that will run an Apache HAWQ or PXF service.
Host Memory Configuration In order to prevent data loss or corruption in an Apache HAWQ cluster, you must configure the memory on each host machine so that the Linux Out-of-Memory (OOM) killer process never kills a HAWQ process due to OOM conditions. (HAWQ applies its own rules to enforce memory restrictions.)
For mission critical deployments of HAWQ, perform these steps on each host machine to configure memory:</description>
    </item>
    
    <item>
      <title>Backing Up and Restoring HAWQ</title>
      <link>https://example.org/admin/backingupandrestoringhawqdatabases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/backingupandrestoringhawqdatabases/</guid>
      <description>This chapter provides information on backing up and restoring databases in HAWQ system.
As an administrator, you will need to back up and restore your database. HAWQ provides three utilities to help you back up your data:
 gpfdist PXF pg_dump  gpfdist and PXF are parallel loading and unloading tools that provide the best performance. You can use pg_dump, a non-parallel utility inherited from PostgreSQL.
In addition, in some situations you should back up your raw data from ETL processes.</description>
    </item>
    
    <item>
      <title>Basic Data Operations</title>
      <link>https://example.org/datamgmt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/datamgmt/</guid>
      <description>This topic describes basic data operations that you perform in HAWQ.
Inserting Rows Use the INSERT command to create rows in a table. This command requires the table name and a value for each column in the table; you may optionally specify the column names in any order. If you do not specify column names, list the data values in the order of the columns in the table, separated by commas.</description>
    </item>
    
    <item>
      <title>Best Practices</title>
      <link>https://example.org/bestpractices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/bestpractices/</guid>
      <description>This chapter provides best practices on using the components and features that are part of a HAWQ system.
 Best Practices for Configuring HAWQ Parameters
This topic provides best practices for configuring the parameters in hawq-site.xml.
 Best Practices for Operating HAWQ
This topic provides best practices for operating HAWQ, including recommendations for stopping, starting and monitoring HAWQ.
 Best Practices for Securing HAWQ
To secure your HAWQ deployment, review the recommendations listed in this topic.</description>
    </item>
    
    <item>
      <title>Changed Behavior with GPORCA</title>
      <link>https://example.org/query/gporca/query-gporca-changed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-changed/</guid>
      <description>When GPORCA is enabled, HAWQ&amp;rsquo;s behavior changes. This topic describes these changes.
 The command CREATE TABLE AS distributes table data randomly if the DISTRIBUTED BY clause is not specified and no primary or unique keys are specified. Statistics are required on the root table of a partitioned table. The ANALYZE command generates statistics on both root and individual partition tables (leaf child tables). See the ROOTPARTITION clause for ANALYZE command.</description>
    </item>
    
    <item>
      <title>Configuring Client Authentication</title>
      <link>https://example.org/clientaccess/client_auth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/client_auth/</guid>
      <description>When a HAWQ system is first initialized, the system contains one predefined superuser role. This role will have the same name as the operating system user who initialized the HAWQ system. This role is referred to as gpadmin. By default, the system is configured to only allow local connections to the database from the gpadmin role. To allow any other roles to connect, or to allow connections from remote hosts, you configure HAWQ to allow such connections.</description>
    </item>
    
    <item>
      <title>Configuring HAWQ/PXF for Secure HDFS</title>
      <link>https://example.org/clientaccess/kerberos-securehdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/kerberos-securehdfs/</guid>
      <description>When Kerberos is enabled for your HDFS filesystem, HAWQ, as an HDFS client, requires a principal and keytab file to authenticate access to HDFS (filesystem) and YARN (resource management). If you have enabled Kerberos at the HDFS filesystem level, you will create and deploy principals for your HDFS cluster, and ensure that Kerberos authentication is enabled and functioning for all HDFS client services, including HAWQ and PXF.
You will perform different procedures depending upon whether you use Ambari to manage your HAWQ cluster or you manage your cluster from the command line.</description>
    </item>
    
    <item>
      <title>Configuring Kerberos User Authentication for HAWQ</title>
      <link>https://example.org/clientaccess/kerberos-userauth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/kerberos-userauth/</guid>
      <description>When Kerberos authentication is enabled at the user level, HAWQ uses the Generic Security Service Application Program Interface (GSSAPI) to provide automatic authentication (single sign-on). When HAWQ uses Kerberos user authentication, both the HAWQ server and those HAWQ users (roles) that use Kerberos authentication require a principal and a keytab. When a user attempts to log in to HAWQ, the HAWQ server uses its Kerberos principal to connect to the Kerberos server, and presents the user&amp;rsquo;s principal for Kerberos validation.</description>
    </item>
    
    <item>
      <title>Configuring PXF</title>
      <link>https://example.org/pxf/configurepxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/configurepxf/</guid>
      <description>This topic describes how to configure the PXF service.
Note: After you make any changes to a PXF configuration file (such as pxf-profiles.xml for adding custom profiles), propagate the changes to all nodes with PXF installed, and then restart the PXF service on all nodes.
Setting up the Java Classpath The classpath for the PXF service is set during the plug-in installation process. Administrators should only modify it when adding new PXF connectors.</description>
    </item>
    
    <item>
      <title>Connecting with psql</title>
      <link>https://example.org/clientaccess/g-connecting-with-psql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/g-connecting-with-psql/</guid>
      <description>Depending on the default values used or the environment variables you have set, the following examples show how to access a database via psql:
$ psql -d gpdatabase -h master_host -p 5432 -U `gpadmin`  $ psql gpdatabase  $ psql  If a user-defined database has not yet been created, you can access the system by connecting to the template1 database. For example:
$ psql template1  After connecting to a database, psql provides a prompt with the name of the database to which psql is currently connected, followed by the string =&amp;gt; (or =# if you are the database superuser).</description>
    </item>
    
    <item>
      <title>Considerations when Using GPORCA</title>
      <link>https://example.org/query/gporca/query-gporca-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-notes/</guid>
      <description>To execute queries optimally with GPORCA, consider certain criteria for the query. 
Ensure the following criteria are met:
 The table does not contain multi-column partition keys. The table does not contain multi-level partitioning. The query does not run against master only tables such as the system table pg_attribute. Statistics have been collected on the root partition of a partitioned table.  If the partitioned table contains more than 20,000 partitions, consider a redesign of the table schema.</description>
    </item>
    
    <item>
      <title>Creating and Managing Databases</title>
      <link>https://example.org/ddl/ddl-database/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-database/</guid>
      <description>Your HAWQ deployment may have multiple databases. This is different from some database management systems (such as Oracle) where the database instance is the database. Although you can create many databases in a HAWQ system, client programs can connect to and access only one database at a time — you cannot cross-query between databases.
About Template Databases Each new database you create is based on a template. HAWQ provides a default database, template1.</description>
    </item>
    
    <item>
      <title>Creating and Managing Schemas</title>
      <link>https://example.org/ddl/ddl-schema/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-schema/</guid>
      <description>Schemas logically organize objects and data in a database. Schemas allow you to have more than one object (such as tables) with the same name in the database without conflict if the objects are in different schemas.
The Default &amp;ldquo;Public&amp;rdquo; Schema Every database has a default schema named public. If you do not create any schemas, objects are created in the public schema. All database roles (users) have CREATE and USAGE privileges in the public schema.</description>
    </item>
    
    <item>
      <title>Creating and Managing Tables</title>
      <link>https://example.org/ddl/ddl-table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-table/</guid>
      <description>HAWQ Tables are similar to tables in any relational database, except that table rows are distributed across the different segments in the system. When you create a table, you specify the table&amp;rsquo;s distribution policy.
Creating a Table The CREATE TABLE command creates a table and defines its structure. When you create a table, you define:
 The columns of the table and their associated data types. See Choosing Column Data Types.</description>
    </item>
    
    <item>
      <title>Creating and Managing Tablespaces</title>
      <link>https://example.org/ddl/ddl-tablespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-tablespace/</guid>
      <description>Tablespaces allow database administrators to have multiple file systems per machine and decide how to best use physical storage to store database objects. They are named locations within a filespace in which you can create objects. Tablespaces allow you to assign different storage for frequently and infrequently used database objects or to control the I/O performance on certain database objects. For example, place frequently-used tables on file systems that use high performance solid-state drives (SSD), and place other tables on standard hard drives.</description>
    </item>
    
    <item>
      <title>Creating and Managing Views</title>
      <link>https://example.org/ddl/ddl-view/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-view/</guid>
      <description>Views enable you to save frequently used or complex queries, then access them in a SELECT statement as if they were a table. A view is not physically materialized on disk: the query runs as a subquery when you access the view.
If a subquery is associated with a single query, consider using the WITH clause of the SELECT command instead of creating a seldom-used view.
Creating Views The CREATE VIEWcommand defines a view of a query.</description>
    </item>
    
    <item>
      <title>Defining Database Objects</title>
      <link>https://example.org/ddl/ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl/</guid>
      <description>This section covers data definition language (DDL) in HAWQ and how to create and manage database objects.
Creating objects in a HAWQ includes making up-front choices about data distribution, storage options, data loading, and other HAWQ features that will affect the ongoing performance of your database system. Understanding the options that are available and how the database will be used will help you make the right decisions.
Most of the advanced HAWQ features are enabled with extensions to the SQL CREATE DDL statements.</description>
    </item>
    
    <item>
      <title>Defining Queries</title>
      <link>https://example.org/query/defining-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/defining-queries/</guid>
      <description>HAWQ is based on the PostgreSQL implementation of the SQL standard. SQL commands are typically entered using the standard PostgreSQL interactive terminal psql, but other programs that have similar functionality can be used as well.
SQL Lexicon SQL is a standard language for accessing databases. The language consists of elements that enable data storage, retrieval, analysis, viewing, and so on. You use SQL commands to construct queries and commands that the HAWQ engine understands.</description>
    </item>
    
    <item>
      <title>Determining The Query Optimizer In Use</title>
      <link>https://example.org/query/gporca/query-gporca-fallback/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-fallback/</guid>
      <description>When GPORCA is enabled, you can determine if HAWQ is using GPORCA or is falling back to the legacy query optimizer. 
These are two ways to determine which query optimizer HAWQ used to execute the query:
 Examine EXPLAIN query plan output for the query. (Your output may include other settings.)
 When GPORCA generates the query plan, the GPORCA version is displayed near the end of the query plan .</description>
    </item>
    
    <item>
      <title>Disabling Kerberos Security</title>
      <link>https://example.org/clientaccess/disable-kerberos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/disable-kerberos/</guid>
      <description>HAWQ supports Kerberos at both the HDFS and/or user authentication levels. You will perform different disable procedures for each.
Disable Kerberized HDFS for HAWQ/PXF You will perform different procedures to disable HAWQ/PXF access to a previously-kerberized HDFS depending upon whether you manage your cluster from the command line or use Ambari to manage your cluster.
Procedure for Ambari-Managed Clusters If you manage your cluster using Ambari, you will disable Kerberos authentication for your cluster as described in the How To Disable Kerberos Hortonworks documentation.</description>
    </item>
    
    <item>
      <title>Enabling Cryptographic Functions for PostgreSQL (pgcrypto)</title>
      <link>https://example.org/plext/using_pgcrypto/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_pgcrypto/</guid>
      <description>pgcrypto is a package extension included in your HAWQ distribution. You must explicitly enable the cryptographic functions to use this extension.
Prerequisites Before you enable the pgcrypto software package, make sure that your HAWQ database is running, you have sourced greenplum_path.sh, and that the $GPHOME environment variable is set.
Enable pgcrypto On every database in which you want to enable pgcrypto, run the following command:
$ psql -d &amp;lt;dbname&amp;gt; -f $GPHOME/share/postgresql/contrib/pgcrypto.</description>
    </item>
    
    <item>
      <title>Enabling GPORCA</title>
      <link>https://example.org/query/gporca/query-gporca-enable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-enable/</guid>
      <description>Precompiled versions of HAWQ that include the GPORCA query optimizer enable it by default, no additional configuration is required. To use the GPORCA query optimizer in a HAWQ built from source, your build must include GPORCA. You must also enable specific HAWQ server configuration parameters at or after install time: 
 Set the optimizer_analyze_root_partition parameter to on to enable statistics collection for the root partition of a partitioned table.</description>
    </item>
    
    <item>
      <title>Establishing a Database Session</title>
      <link>https://example.org/clientaccess/g-establishing-a-database-session/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/g-establishing-a-database-session/</guid>
      <description>Users can connect to HAWQ using a PostgreSQL-compatible client program, such as psql. Users and administrators always connect to HAWQ through the master; the segments cannot accept client connections.
In order to establish a connection to the HAWQ master, you will need to know the following connection information and configure your client program accordingly.
   Connection Parameter Description Environment Variable     Application name The application name that is connecting to the database.</description>
    </item>
    
    <item>
      <title>Example - Setting up an MIT KDC Server</title>
      <link>https://example.org/clientaccess/kerberos-mitkdc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/kerberos-mitkdc/</guid>
      <description>Follow this procedure to install and configure a Kerberos KDC server on a Red Hat Enterprise Linux host. The KDC server resides on the host named &amp;lt;kdc-server&amp;gt;.
 Log in to the Kerberos KDC Server system as a superuser:
$ ssh root@&amp;lt;kdc-server&amp;gt; root@kdc-server$  Install the Kerberos server packages:
root@kdc-server$ yum install krb5-libs krb5-server krb5-workstation  Define the Kerberos realm for your cluster by editting the /etc/krb5.conf configuration file. The following example configures a Kerberos server with a realm named REALM.</description>
    </item>
    
    <item>
      <title>Expanding a Cluster</title>
      <link>https://example.org/admin/clusterexpansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/clusterexpansion/</guid>
      <description>Apache HAWQ supports dynamic node expansion. You can add segment nodes while HAWQ is running without having to suspend or terminate cluster operations.
Note: This topic describes how to expand a cluster using the command-line interface. If you are using Ambari to manage your HAWQ cluster, see Expanding the HAWQ Cluster in Managing HAWQ Using Ambari
Guidelines for Cluster Expansion This topic provides some guidelines around expanding your HAWQ cluster.</description>
    </item>
    
    <item>
      <title>GPORCA Features and Enhancements</title>
      <link>https://example.org/query/gporca/query-gporca-features/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-features/</guid>
      <description>GPORCA includes enhancements for specific types of queries and operations. GPORCA also includes these optimization enhancements:
 Improved join ordering Join-Aggregate reordering Sort order optimization Data skew estimates included in query optimization  Queries Against Partitioned Tables GPORCA includes these enhancements for queries against partitioned tables:
 Partition elimination is improved. Query plan can contain the Partition selector operator. Partitions are not enumerated in EXPLAIN plans.
For queries that involve static partition selection where the partitioning key is compared to a constant, GPORCA lists the number of partitions to be scanned in the EXPLAIN output under the Partition Selector operator.</description>
    </item>
    
    <item>
      <title>GPORCA Limitations</title>
      <link>https://example.org/query/gporca/query-gporca-limitations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-limitations/</guid>
      <description>There are limitations in HAWQ when GPORCA is enabled. GPORCA and the legacy query optimizer currently coexist in HAWQ because GPORCA does not support all HAWQ features. 
Unsupported SQL Query Features These HAWQ features are unsupported when GPORCA is enabled:
 Indexed expressions PERCENTILE window function External parameters SortMergeJoin (SMJ) Ordered aggregations These analytics extensions:  CUBE Multiple grouping sets  These scalar operators:  ROW ROWCOMPARE FIELDSELECT  Multiple DISTINCT qualified aggregate functions Inverse distribution functions  Performance Regressions When GPORCA is enabled in HAWQ, the following features are known performance regressions:</description>
    </item>
    
    <item>
      <title>Getting Started with HAWQ</title>
      <link>https://example.org/tutorial/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/overview/</guid>
      <description>Overview This tutorial provides a quick introduction to get you up and running with your HAWQ installation. You will be introduced to basic HAWQ functionality, including cluster management, database creation, and simple querying. You will also become acquainted with using the HAWQ Extension Framework (PXF) to access and query external HDFS data sources.
Prerequisites Ensure that you have a running HAWQ 2.x single or multi-node cluster. You may choose to use a:</description>
    </item>
    
    <item>
      <title>HAWQ Administrative Log Files</title>
      <link>https://example.org/admin/logfiles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/logfiles/</guid>
      <description>Log files are files that include messages and other information about your HAWQ deployment, including the database and utilities. HAWQ administrative log files reside in pre-defined or configured locations on the local file system of the HAWQ node. These log files are distinctly located, formatted, configured, and managed.
Every database instance in HAWQ (master, standby, and segments) runs a PostgreSQL database server with its own server log file. You generate log files when you invoke HAWQ management utilities directly, or indirectly via Ambari management operations.</description>
    </item>
    
    <item>
      <title>HAWQ Client Applications</title>
      <link>https://example.org/clientaccess/g-hawq-database-client-applications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/g-hawq-database-client-applications/</guid>
      <description>HAWQ is pre-installed with a number of client utility applications. You can also run client applications in your HAWQ deployment that were created using standard ODBC/JDBC Application Interfaces.
Alternatively, you may want to provide access to your HAWQ master node from a Linux client outside of your HAWQ cluster. One option to enable this access is to install the full HAWQ binary on the client node. A second option is to create a minimal psql client package from an existing HAWQ installation which you could then install on a non-HAWQ client.</description>
    </item>
    
    <item>
      <title>HAWQ Database Drivers and APIs</title>
      <link>https://example.org/clientaccess/g-database-application-interfaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/g-database-application-interfaces/</guid>
      <description>You may want to connect your existing Business Intelligence (BI) or Analytics applications with HAWQ. The database application programming interfaces most commonly used with HAWQ are the Postgres and ODBC and JDBC APIs.
HAWQ provides the following connectivity tools for connecting to the database:
 ODBC driver JDBC driver libpq - PostgreSQL C API  HAWQ Drivers ODBC and JDBC drivers for HAWQ are available as a separate download from Pivotal Network Pivotal Network.</description>
    </item>
    
    <item>
      <title>HAWQ Filespaces and High Availability Enabled HDFS</title>
      <link>https://example.org/admin/hawqfilespacesandhighavailabilityenabledhdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/hawqfilespacesandhighavailabilityenabledhdfs/</guid>
      <description>If you initialized HAWQ without the HDFS High Availability (HA) feature, you can enable it by using the following procedure.
Enabling the HDFS NameNode HA Feature To enable the HDFS NameNode HA feature for use with HAWQ, you need to perform the following tasks:
 Enable high availability in your HDFS cluster. Collect information about the target filespace. Stop the HAWQ cluster and backup the catalog (Note: Ambari users must perform this manual step.</description>
    </item>
    
    <item>
      <title>HAWQ Reference</title>
      <link>https://example.org/reference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/reference/</guid>
      <description>This section provides a complete reference to HAWQ SQL commands, management utilities, configuration parameters, environment variables, and database objects.
 Server Configuration Parameter Reference
This section describes all server configuration guc/parameters that are available in HAWQ.
 HDFS Configuration Reference
This reference page describes HDFS configuration values that are configured for HAWQ either within hdfs-site.xml, core-site.xml, or hdfs-client.xml.
 Environment Variables
This topic contains a reference of the environment variables that you set for HAWQ.</description>
    </item>
    
    <item>
      <title>HAWQ packcore utility</title>
      <link>https://example.org/admin/packcore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/packcore/</guid>
      <description>Core file A core file is a disk file that records the memory image of a running process in the event the process crashes or terminates abruptly. The information in this image is useful for debugging the state of a process at the time when it was terminated.
Packcore The packcore utility helps pack a core file with its context, including the executable, application, and shared system libraries from the current environment.</description>
    </item>
    
    <item>
      <title>High Availability in HAWQ</title>
      <link>https://example.org/admin/highavailability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/highavailability/</guid>
      <description>A HAWQ cluster can be made highly available by providing fault-tolerant hardware, by enabling HAWQ or HDFS high-availability features, and by performing regular monitoring and maintenance procedures to ensure the health of all system components.
Hardware components eventually fail either due to normal wear or to unexpected circumstances. Loss of power can lead to temporarily unavailable components. You can make a system highly available by providing redundant standbys for components that can fail so services can continue uninterrupted when a failure does occur.</description>
    </item>
    
    <item>
      <title>Identifying HAWQ Table HDFS Files</title>
      <link>https://example.org/ddl/locate-table-hdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/locate-table-hdfs/</guid>
      <description>You can determine the HDFS location of the data file(s) associated with a specific HAWQ table using the HAWQ filespace HDFS location, the table identifier, and the identifiers for the tablespace and database in which the table resides.
The number of HDFS data files associated with a HAWQ table is determined by the distribution mechanism (hash or random) identified when the table was first created or altered.
Only an HDFS or HAWQ superuser may access HAWQ table HDFS files.</description>
    </item>
    
    <item>
      <title>Installing PXF Plug-ins</title>
      <link>https://example.org/pxf/installpxfplugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/installpxfplugins/</guid>
      <description>This topic describes how to install the built-in PXF service plug-ins that are required to connect PXF to HDFS, Hive, HBase, JDBC, and JSON.
Note: PXF requires that you run Tomcat on the host machine. Tomcat reserves ports 8005, 8080, and 8009. If you have configured Oozie JXM reporting on a host that will run PXF, make sure that the reporting service uses a port other than 8005. This helps to prevent port conflict errors from occurring when you start the PXF service.</description>
    </item>
    
    <item>
      <title>Introducing the HAWQ Operating Environment</title>
      <link>https://example.org/admin/setuphawqopenv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/setuphawqopenv/</guid>
      <description>Before invoking operations on a HAWQ cluster, you must set up your HAWQ environment. This set up is required for both administrative and non-administrative HAWQ users.
Procedure: Setting Up Your HAWQ Operating Environment HAWQ installs a script that you can use to set up your HAWQ cluster environment. The greenplum_path.sh script, located in your HAWQ root install directory, sets $PATH and other environment variables to find HAWQ files. Most importantly, greenplum_path.</description>
    </item>
    
    <item>
      <title>Lesson 1 - Runtime Environment</title>
      <link>https://example.org/tutorial/gettingstarted/introhawqenv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/introhawqenv/</guid>
      <description>This section introduces you to the HAWQ runtime environment. You will examine your HAWQ installation, set up your HAWQ environment, and execute HAWQ management commands. If installed in your environment, you will also explore the Ambari management console.
Prerequisites  Install a HAWQ commercial product distribution or HAWQ sandbox virtual machine or docker environment, or build and install HAWQ from source. Ensure that your HAWQ installation is configured appropriately.</description>
    </item>
    
    <item>
      <title>Lesson 2 - Cluster Administration</title>
      <link>https://example.org/tutorial/gettingstarted/basichawqadmin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/basichawqadmin/</guid>
      <description>The HAWQ gpadmin administrative user has super-user capabilities on all HAWQ databases and HAWQ cluster management commands.
HAWQ configuration parameters affect the behaviour of both the HAWQ cluster and individual HAWQ nodes.
This lesson introduces basic HAWQ cluster administration tasks. You will view and update HAWQ configuration parameters.
Note: Before installing HAWQ, you or your administrator choose to configure and manage the HAWQ cluster either using the command line or using the Ambari UI.</description>
    </item>
    
    <item>
      <title>Lesson 3 - Database Administration</title>
      <link>https://example.org/tutorial/gettingstarted/basicdbadmin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/basicdbadmin/</guid>
      <description>The HAWQ gpadmin user and other users who are granted the necessary privileges can execute SQL commands to create HAWQ databases and tables. These commands may be invoked via scripts, programs, and from the psql client utility.
This lesson introduces basic HAWQ database administration commands and tasks using psql. You will create a database and a simple table, and add data to and query the table.
 Prerequisites Ensure that you have Set Up your HAWQ Runtime Environment and that your HAWQ cluster is up and running.</description>
    </item>
    
    <item>
      <title>Lesson 4 - Sample Data Set and HAWQ Schemas</title>
      <link>https://example.org/tutorial/gettingstarted/dataandscripts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/dataandscripts/</guid>
      <description>The sample Retail demo data set used in the tutorial exercises models an online retail store operation. The store carries different categories of products. Customers order the products. The company delivers the products to the customers.
This and later exercises operate on this example data set. The data set is provided in a set of gzip&amp;rsquo;d .tsv (tab-separated values) text files. The exercises also reference scripts and other supporting files that operate on the data set.</description>
    </item>
    
    <item>
      <title>Lesson 5 - HAWQ Tables</title>
      <link>https://example.org/tutorial/gettingstarted/introhawqtbls/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/introhawqtbls/</guid>
      <description>HAWQ writes data to, and reads data from, HDFS natively. HAWQ tables are similar to tables in any relational database, except that table rows (data) are distributed across the different segments in the cluster.
In this exercise, you will run scripts that use the SQL CREATE TABLE command to create HAWQ tables. You will load the Retail demo fact data into the HAWQ tables using the SQL COPY command. You will then perform simple and complex queries on the data.</description>
    </item>
    
    <item>
      <title>Lesson 6 - HAWQ Extension Framework (PXF)</title>
      <link>https://example.org/tutorial/gettingstarted/intropxfhdfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/tutorial/gettingstarted/intropxfhdfs/</guid>
      <description>Data in many HAWQ deployments may already reside in external sources. The HAWQ Extension Framework (PXF) provides access to this external data via built-in connectors called plug-ins. PXF plug-ins facilitate mapping a data source to a HAWQ external table definition. PXF is installed with HDFS, Hive, HBase, and JSON plug-ins.
In this exercise, you use the PXF HDFS plug-in to:
 Create PXF external table definitions Perform queries on the data you loaded into HDFS Run more complex queries on HAWQ and PXF tables  Prerequisites Ensure that you have:</description>
    </item>
    
    <item>
      <title>Managing HAWQ Using Ambari</title>
      <link>https://example.org/admin/ambari-admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/ambari-admin/</guid>
      <description>Ambari provides an easy interface to perform some of the most common HAWQ and PXF Administration Tasks.
Integrating YARN for Resource Management HAWQ supports integration with YARN for global resource management. In a YARN managed environment, HAWQ can request resources (containers) dynamically from YARN, and return resources when HAWQ’s workload is not heavy.
See also Integrating YARN with HAWQ for command-line instructions and additional details about using HAWQ with YARN.</description>
    </item>
    
    <item>
      <title>Managing Resources</title>
      <link>https://example.org/resourcemgmt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/resourcemgmt/</guid>
      <description>This section describes how to use HAWQ&amp;rsquo;s resource management features:
 How HAWQ Manages Resources Best Practices for Configuring Resource Management Configuring Resource Management Integrating YARN with HAWQ Working with Hierarchical Resource Queues Analyzing Resource Manager Status  </description>
    </item>
    
    <item>
      <title>Monitoring a HAWQ System</title>
      <link>https://example.org/admin/monitor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/monitor/</guid>
      <description>You can monitor a HAWQ system using a variety of tools included with the system or available as add-ons.
Observing the HAWQ system day-to-day performance helps administrators understand the system behavior, plan workflow, and troubleshoot problems. This chapter discusses tools for monitoring database performance and activity.
Also, be sure to review Recommended Monitoring and Maintenance Tasks for monitoring activities you can script to quickly detect problems in the system.</description>
    </item>
    
    <item>
      <title>Overview of GPORCA</title>
      <link>https://example.org/query/gporca/query-gporca-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/gporca/query-gporca-overview/</guid>
      <description>GPORCA extends the planning and optimization capabilities of the HAWQ legacy optimizer.  GPORCA is extensible and achieves better optimization in multi-core architecture environments. When GPORCA is available in your HAWQ installation and enabled, HAWQ uses GPORCA to generate an execution plan for a query when possible.
GPORCA also enhances HAWQ query performance tuning in the following areas:
 Queries against partitioned tables Queries that contain a common table expression (CTE) Queries that contain subqueries  The legacy and GPORCA query optimizers coexist in HAWQ.</description>
    </item>
    
    <item>
      <title>Overview of HAWQ Authorization</title>
      <link>https://example.org/clientaccess/hawq-access-checks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/hawq-access-checks/</guid>
      <description>Native HAWQ authorization provides SQL standard authorization at the database and table level for specific users/roles using the GRANT and REVOKE SQL commands. HAWQ integration with Ranger provides policy-based authorization, enabling you to identify the conditions under which a user and/or group can access individual HAWQ resources, including the operations permitted on those resources.
Native HAWQ and Ranger authorization are mutually exclusive.
Native HAWQ and Ranger authorization share pg_hba.conf-based user authentication.</description>
    </item>
    
    <item>
      <title>Overview of Ranger Policy Management</title>
      <link>https://example.org/ranger/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ranger/</guid>
      <description>HAWQ supports using Apache Ranger for authorizing user access to HAWQ resources. Using Ranger enables you to manage all of your Hadoop components&amp;rsquo; authorization policies using the same user interface, policy store, and auditing stores.
See the Apache Ranger documentation for more information about the core functionality of Ranger.
Policy Management Architecture Each HAWQ installation includes a Ranger plug-in service to support Ranger Policy management. The Ranger plug-in service implements the Ranger REST API to bridge all requests between the Ranger Policy Manager and a HAWQ instance.</description>
    </item>
    
    <item>
      <title>PXF External Tables and API</title>
      <link>https://example.org/pxf/pxfexternaltableandapireference/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/pxfexternaltableandapireference/</guid>
      <description>You can use the PXF API to create your own connectors to access any other type of parallel data store or processing engine.
The PXF Java API lets you extend PXF functionality and add new services and formats without changing HAWQ. The API includes three classes that are extended to allow HAWQ to access an external data source: Fragmenter, Accessor, and Resolver.
The Fragmenter produces a list of data fragments that can be read in parallel from the data source.</description>
    </item>
    
    <item>
      <title>Partitioning Large Tables</title>
      <link>https://example.org/ddl/ddl-partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-partition/</guid>
      <description>Table partitioning enables supporting very large tables, such as fact tables, by logically dividing them into smaller, more manageable pieces. Partitioned tables can improve query performance by allowing the HAWQ query optimizer to scan only the data needed to satisfy a given query instead of scanning all the contents of a large table.
Partitioning does not change the physical distribution of table data across the segments. Table distribution is physical: HAWQ physically divides partitioned tables and non-partitioned tables across segments to enable parallel query processing.</description>
    </item>
    
    <item>
      <title>Pivotal HDB 2.3.0 Release Notes</title>
      <link>https://example.org/hawq230releasenotes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/hawq230releasenotes/</guid>
      <description>Pivotal HDB 2.3.0 is a minor release of the product and is based on Apache HAWQ&amp;reg; (Incubating). This release includes HAWQ Ranger High Availability and Kerberos support, Beta support for the PXF HiveVectorizedORC and JDBC profiles, and bug fixes.
Supported Platforms The supported platform for running Pivotal HDB 2.3.0 comprises:
 Red Hat Enterprise Linux (RHEL) 6.4+, 7.2+ (64-bit) (See note in Known Issues and Limitations for kernel limitations.) Hortonworks Data Platform (HDP) 2.</description>
    </item>
    
    <item>
      <title>Query Performance</title>
      <link>https://example.org/query/query-performance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/query-performance/</guid>
      <description>HAWQ dynamically allocates resources to queries. Query performance depends on several factors such as data locality, number of virtual segments used for the query and general cluster health.
 Dynamic Partition Elimination
In HAWQ, values available only when a query runs are used to dynamically prune partitions, which improves query processing speed. Enable or disable dynamic partition elimination by setting the server configuration parameter gp_dynamic_partition_pruning to ON or OFF; it is ON by default.</description>
    </item>
    
    <item>
      <title>Query Profiling</title>
      <link>https://example.org/query/query-profiling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/query-profiling/</guid>
      <description>Examine the query plans of poorly performing queries to identify possible performance tuning opportunities.
HAWQ devises a query plan for each query. Choosing the right query plan to match the query and data structure is necessary for good performance. A query plan defines how HAWQ will run the query in the parallel execution environment.
The query optimizer uses data statistics maintained by the database to choose a query plan with the lowest possible cost.</description>
    </item>
    
    <item>
      <title>Querying Data</title>
      <link>https://example.org/query/query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/query/</guid>
      <description>This topic provides information about using SQL in HAWQ databases.
You enter SQL statements called queries to view and analyze data in a database using the psql interactive SQL client and other client tools.
Note: HAWQ queries timeout after a period of 600 seconds. For this reason, long-running queries may appear to hang until results are processed or until the timeout period expires.
 About HAWQ Query Processing
This topic provides an overview of how HAWQ processes queries.</description>
    </item>
    
    <item>
      <title>Recommended Monitoring and Maintenance Tasks</title>
      <link>https://example.org/admin/recommendedmonitoringtasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/recommendedmonitoringtasks/</guid>
      <description>This section lists monitoring and maintenance activities recommended to ensure high availability and consistent performance of your HAWQ cluster.
The tables in the following sections suggest activities that a HAWQ System Administrator can perform periodically to ensure that all components of the system are operating optimally. Monitoring activities help you to detect and diagnose problems early. Maintenance activities help you to keep the system up-to-date and avoid deteriorating performance, for example, from bloated system tables or diminishing free disk space.</description>
    </item>
    
    <item>
      <title>Removing a Node</title>
      <link>https://example.org/admin/clustershrink/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/clustershrink/</guid>
      <description>This topic outlines the proper procedure for removing a node from a HAWQ cluster.
In general, you should not need to remove nodes manually from running HAWQ clusters. HAWQ isolates any nodes that HAWQ detects as failing due to hardware or other types of errors.
Guidelines for Removing a Node If you do need to remove a node from a HAWQ cluster, keep in mind the following guidelines around removing nodes:</description>
    </item>
    
    <item>
      <title>Routine System Maintenance Tasks</title>
      <link>https://example.org/admin/maintain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/maintain/</guid>
      <description>Overview To keep a HAWQ system running efficiently, the database must be regularly cleared of expired data and the table statistics must be updated so that the query optimizer has accurate information.
HAWQ requires that certain tasks be performed regularly to achieve optimal performance. The tasks discussed here are required, but database administrators can automate them using standard UNIX tools such as cron scripts. An administrator sets up the appropriate scripts and checks that they execute successfully.</description>
    </item>
    
    <item>
      <title>Running a HAWQ Cluster</title>
      <link>https://example.org/admin/runninghawq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/runninghawq/</guid>
      <description>This section provides information for system administrators responsible for administering a HAWQ deployment.
You should have some knowledge of Linux/UNIX system administration, database management systems, database administration, and structured query language (SQL) to administer a HAWQ cluster. Because HAWQ is based on PostgreSQL, you should also have some familiarity with PostgreSQL. The HAWQ documentation calls out similarities between HAWQ and PostgreSQL features throughout.
HAWQ Users HAWQ supports users with both administrative and operating privileges.</description>
    </item>
    
    <item>
      <title>Select HAWQ Host Machines</title>
      <link>https://example.org/install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/install/</guid>
      <description>Before you begin to install HAWQ, follow these steps to select and prepare the host machines.
Complete this procedure for all HAWQ deployments:
 Choose the host machines that will host a HAWQ segment. Keep in mind these restrictions and requirements:  Each host must meet the system requirements for the version of HAWQ you are installing. Each HAWQ segment must be co-located on a host that runs an HDFS DataNode.</description>
    </item>
    
    <item>
      <title>Starting and Stopping HAWQ</title>
      <link>https://example.org/admin/startstop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/startstop/</guid>
      <description>In a HAWQ DBMS, the database server instances (the master and all segments) are started or stopped across all of the hosts in the system in such a way that they can work together as a unified DBMS.
Because a HAWQ system is distributed across many machines, the process for starting and stopping a HAWQ system is different than the process for starting and stopping a regular PostgreSQL DBMS.</description>
    </item>
    
    <item>
      <title>Table Storage Model and Distribution Policy</title>
      <link>https://example.org/ddl/ddl-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/ddl/ddl-storage/</guid>
      <description>HAWQ supports several storage models and a mix of storage models. When you create a table, you choose how to store its data. This topic explains the options for table storage and how to choose the best storage model for your workload.
Note: To simplify the creation of database tables, you can specify the default values for some table storage options with the HAWQ server configuration parameter gp_default_storage_options.
Row-Oriented Storage HAWQ provides storage orientation models of either row-oriented or Parquet tables.</description>
    </item>
    
    <item>
      <title>Troubleshooting</title>
      <link>https://example.org/troubleshooting/troubleshooting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/troubleshooting/troubleshooting/</guid>
      <description>This chapter describes how to resolve common problems and errors that occur in a HAWQ system.
Query Performance Issues Problem: Query performance is slow.
Cause: There can be multiple reasons why a query might be performing slowly. For example, the locality of data distribution, the number of virtual segments, or the number of hosts used to execute the query can all affect its performance. The following procedure describes how to investigate query performance issues.</description>
    </item>
    
    <item>
      <title>Troubleshooting Connection Problems</title>
      <link>https://example.org/clientaccess/g-troubleshooting-connection-problems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/g-troubleshooting-connection-problems/</guid>
      <description>A number of things can prevent a client application from successfully connecting to HAWQ. This topic explains some of the common causes of connection problems and how to correct them.
   Problem Solution     No pg_hba.conf entry for host or user To enable HAWQ to accept remote client connections, you must configure your HAWQ master instance so that connections are allowed from the client hosts and database users that will be connecting to HAWQ.</description>
    </item>
    
    <item>
      <title>Troubleshooting PXF</title>
      <link>https://example.org/pxf/troubleshootingpxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/troubleshootingpxf/</guid>
      <description>PXF Errors The following table lists some common errors encountered while using PXF:
 Table 1. PXF Errors and Explanation    Error Common Explanation    ERROR: invalid URI pxf://localhost:51200/demo/file1: missing options section LOCATION does not include options after the file name: &amp;lt;path&amp;gt;?&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;&amp;amp;&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;...  ERROR: protocol &amp;quot;pxf&amp;quot; does not exist HAWQ is not compiled with PXF protocol. It requires the GPSQL version of HAWQ  ERROR: remote component error (0) from &#39;&amp;lt;x&amp;gt;&#39;: There is no pxf servlet listening on the host and port specified in the external table url.</description>
    </item>
    
    <item>
      <title>Understanding the Fault Tolerance Service</title>
      <link>https://example.org/admin/faulttolerance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/faulttolerance/</guid>
      <description>The fault tolerance service (FTS) enables HAWQ to continue operating in the event that a segment node fails. The fault tolerance service runs automatically and requires no additional configuration requirements.
Each segment runs a resource manager process that periodically sends (by default, every 30 seconds) the segment’s status to the master&amp;rsquo;s resource manager process. This interval is controlled by the hawq_rm_segment_heartbeat_interval server configuration parameter.
When a segment encounters a critical error&amp;ndash; for example, a temporary directory on the segment fails due to a hardware error&amp;ndash; the segment reports that there is temporary directory failure to the HAWQ master through a heartbeat report.</description>
    </item>
    
    <item>
      <title>Using Functions and Operators</title>
      <link>https://example.org/query/functions-operators/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/query/functions-operators/</guid>
      <description>HAWQ evaluates functions and operators used in SQL expressions.
Using Functions in HAWQ In HAWQ, functions can only be run on master.

Table 1. Functions in HAWQ
   Function Type HAWQ Support Description Comments     IMMUTABLE Yes Relies only on information directly in its argument list. Given the same argument values, always returns the same result.     STABLE Yes, in most cases Within a single table scan, returns the same result for same argument values, but results change across SQL statements.</description>
    </item>
    
    <item>
      <title>Using HAWQ Built-In Languages</title>
      <link>https://example.org/plext/builtin_langs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/builtin_langs/</guid>
      <description>This section provides an introduction to using the HAWQ built-in languages.
HAWQ supports user-defined functions created with the SQL and C built-in languages. HAWQ also supports user-defined aliases for internal functions.
Enabling Built-in Language Support Support for SQL and C language user-defined functions and aliasing of internal functions is enabled by default for all HAWQ databases.
Defining SQL Functions SQL functions execute an arbitrary list of SQL statements. The SQL statements in the body of a SQL function must be separated by semicolons.</description>
    </item>
    
    <item>
      <title>Using HAWQ Native Authorization</title>
      <link>https://example.org/clientaccess/roles_privs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/roles_privs/</guid>
      <description>The HAWQ authorization mechanism stores roles and permissions to access database objects in the database and is administered using SQL statements or command-line utilities.
HAWQ manages database access permissions using roles. The concept of roles subsumes the concepts of users and groups. A role can be a database user, a group, or both. Roles can own database objects (for example, tables) and can assign privileges on those objects to other roles to control access to the objects.</description>
    </item>
    
    <item>
      <title>Using Kerberos Authentication</title>
      <link>https://example.org/clientaccess/kerberos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/kerberos/</guid>
      <description>Kerberos is an encrpyted network authentication protocol for client/server applications. Kerberos is a complex subsystem. Detailing how to install and configure Kerberos itself is beyond the scope of this document. You should familiarize yourself with Kerberos concepts before configuring Kerberos for your HAWQ cluster. For more information about Kerberos, see http://web.mit.edu/kerberos/.
HAWQ supports Kerberos at both the HDFS and/or user authentication levels. You will perform distinct configuration procedures for each.</description>
    </item>
    
    <item>
      <title>Using LDAP Authentication with TLS/SSL</title>
      <link>https://example.org/clientaccess/ldap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/clientaccess/ldap/</guid>
      <description>You can control access to HAWQ with an LDAP server and, optionally, secure the connection with encryption by adding parameters to pg_hba.conf file entries.
HAWQ supports LDAP authentication with the TLS/SSL protocol to encrypt communication with an LDAP server:
 LDAP authentication with STARTTLS and TLS protocol – STARTTLS starts with a clear text connection (no encryption) and upgrades it to a secure connection (with encryption). LDAP authentication with a secure connection and TLS/SSL (LDAPS) – HAWQ uses the TLS or SSL protocol based on the protocol that is used by the LDAP server.</description>
    </item>
    
    <item>
      <title>Using Languages and Extensions in HAWQ</title>
      <link>https://example.org/plext/usingprocedurallanguages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/usingprocedurallanguages/</guid>
      <description>HAWQ supports user-defined functions that are created with the SQL and C built-in languages, and also supports user-defined aliases for internal functions.
HAWQ also supports user-defined functions written in languages other than SQL and C. These other languages are generically called procedural languages (PLs) and are extensions to the core HAWQ functionality. HAWQ specifically supports the PL/Java, PL/Perl, PL/pgSQL, PL/Python, and PL/R procedural languages.
HAWQ additionally provides the pgcrypto extension for password hashing and data encryption.</description>
    </item>
    
    <item>
      <title>Using Master Mirroring</title>
      <link>https://example.org/admin/mastermirroring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/mastermirroring/</guid>
      <description>There are two masters in a HAWQ cluster&amp;ndash; a primary master and a standby master. Clients connect to the primary master and queries can be executed only on the primary master.
You deploy a backup or mirror of the master instance on a separate host machine from the primary master so that the cluster can tolerate a single host failure. A backup master or standby master serves as a warm standby if the primary master becomes non-operational.</description>
    </item>
    
    <item>
      <title>Using PL/Java</title>
      <link>https://example.org/plext/using_pljava/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_pljava/</guid>
      <description>This section provides an overview of the HAWQ PL/Java procedural language extension.
About PL/Java With the PL/Java extension, you can write Java methods using your favorite Java IDE and invoke the methods from PostgreSQL user-defined functions (UDFs).
The HAWQ PL/Java package is based on the open source PL/Java 1.4.0 and provides the following features:
 PL/Java function execution with Java 1.6 or 1.7. Standardized mappings of Java and PostgreSQL parameters and results.</description>
    </item>
    
    <item>
      <title>Using PL/Perl</title>
      <link>https://example.org/plext/using_plperl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_plperl/</guid>
      <description>This section contains an overview of the HAWQ PL/Perl language extension.
Enabling PL/Perl If PL/Perl is enabled during HAWQ build time, HAWQ installs the PL/Perl language extension automatically. To use PL/Perl, you must enable it on specific databases.
On every database where you want to enable PL/Perl, connect to the database using the psql client.
$ psql -d &amp;lt;dbname&amp;gt;  Replace &amp;lt;dbname&amp;gt; with the name of the target database.</description>
    </item>
    
    <item>
      <title>Using PL/Python in HAWQ</title>
      <link>https://example.org/plext/using_plpython/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_plpython/</guid>
      <description>This section provides an overview of the HAWQ PL/Python procedural language extension.
About HAWQ PL/Python PL/Python is embedded in your HAWQ product distribution or within your HAWQ build if you chose to enable it as a build option.
With the HAWQ PL/Python extension, you can write user-defined functions in Python that take advantage of Python features and modules, enabling you to quickly build robust HAWQ database applications.
HAWQ uses the system Python installation.</description>
    </item>
    
    <item>
      <title>Using PL/R in HAWQ</title>
      <link>https://example.org/plext/using_plr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_plr/</guid>
      <description>PL/R is a procedural language. With the HAWQ PL/R extension, you can write database functions in the R programming language and use R packages that contain R functions and data sets.
Note: To use PL/R in HAWQ, R must be installed on each node in your HAWQ cluster. Additionally, you must install the PL/R package on an existing HAWQ deployment or have specified PL/R as a build option when compiling HAWQ.</description>
    </item>
    
    <item>
      <title>Using PL/pgSQL in HAWQ</title>
      <link>https://example.org/plext/using_plpgsql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/plext/using_plpgsql/</guid>
      <description>PL/pgSQL is a trusted procedural language that is automatically installed and registered in all HAWQ databases. With PL/pgSQL, you can:
 Create functions Add control structures to the SQL language Perform complex computations Use all of the data types, functions, and operators defined in SQL  SQL is the language most relational databases use as a query language. While it is portable and easy to learn, every SQL statement is individually executed by the database server.</description>
    </item>
    
    <item>
      <title>Using PXF with Unmanaged Data</title>
      <link>https://example.org/pxf/hawqextensionframeworkpxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/hawqextensionframeworkpxf/</guid>
      <description>HAWQ Extension Framework (PXF) is an extensible framework that allows HAWQ to query external system data. PXF includes built-in connectors for accessing data inside HDFS files, Hive tables, and HBase tables. PXF also integrates with HCatalog to query Hive tables directly.
PXF allows users to create custom connectors to access other parallel data stores or processing engines. To create these connectors using Java plug-ins, see the PXF External Tables and API.</description>
    </item>
    
    <item>
      <title>Using Profiles to Read and Write Data</title>
      <link>https://example.org/pxf/readwritepxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/readwritepxf/</guid>
      <description>PXF profiles are collections of common metadata attributes that can be used to simplify the reading and writing of data. You can use any of the built-in profiles that come with PXF or you can create your own.
For example, if you are writing single line records to text files on HDFS, you could use the built-in HdfsTextSimple profile. You specify this profile when you create the PXF external table used to write the data to HDFS.</description>
    </item>
    
    <item>
      <title>Using the Ambari REST API</title>
      <link>https://example.org/admin/ambari-rest-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/admin/ambari-rest-api/</guid>
      <description>You can monitor and manage the resources in your HAWQ cluster using the Ambari REST API. In addition to providing access to the metrics information in your cluster, the API supports viewing, creating, deleting, and updating cluster resources.
This section will provide an introduction to using the Ambari REST APIs for HAWQ-related cluster management activities.
Refer to Ambari API Reference v1 for the official Ambari API documentation, including full REST resource definitions and response semantics.</description>
    </item>
    
    <item>
      <title>What is HAWQ?</title>
      <link>https://example.org/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/overview/</guid>
      <description>HAWQ is a Hadoop native SQL query engine that combines the key technological advantages of MPP database with the scalability and convenience of Hadoop. HAWQ reads data from and writes data to HDFS natively.
HAWQ delivers industry-leading performance and linear scalability. It provides users the tools to confidently and successfully interact with petabyte range data sets. HAWQ provides users with a complete, standards compliant SQL interface. More specifically, HAWQ has the following features:</description>
    </item>
    
    <item>
      <title>Writing Data to HDFS</title>
      <link>https://example.org/pxf/hdfswritablepxf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://example.org/pxf/hdfswritablepxf/</guid>
      <description>The PXF HDFS plug-in supports writable external tables using the HdfsTextSimple and SequenceWritable profiles. You might create a writable table to export data from a HAWQ internal table to binary or text HDFS files.
Use the HdfsTextSimple profile when writing text data. Use the SequenceWritable profile when dealing with binary data.
This section describes how to use these PXF profiles to create writable external tables.
Note: Tables that you create with writable profiles can only be used for INSERT operations.</description>
    </item>
    
  </channel>
</rss>